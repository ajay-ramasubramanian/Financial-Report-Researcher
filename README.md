# ğŸ“Š Financial Report Researcher

> **Enterprise-grade RAG system for SEC 10-K and 10-Q filing analysis with local LLM inference**

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-green.svg)](https://fastapi.tiangolo.com/)
[![ChromaDB](https://img.shields.io/badge/ChromaDB-0.4+-orange.svg)](https://www.trychroma.com/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

A Retrieval-Augmented Generation (RAG) application that automatically downloads SEC EDGAR filings, processes them with semantic chunking, and enables natural language querying through locally-hosted LLMs via Ollama.

![Financial Report Analyzer UI](docs/ui_screenshot.png)

---

## âœ¨ Key Features

| Feature | Description |
|---------|-------------|
| **ğŸ”„ Automated SEC Download** | Fetches 10-K and 10-Q filings directly from SEC EDGAR with 2-year lookback |
| **ğŸ§  Semantic Chunking** | Section-aware processing (MD&A, Risk Factors, Financial Statements) |
| **ğŸ”’ 100% Local** | No data leaves your machine - embeddings and LLM run via Ollama |
| **ğŸ“Š Chunk Monitoring** | See exactly which filing sections are retrieved for each query |
| **âš¡ REST API** | FastAPI backend with streaming support and OpenAPI docs |
| **ğŸ–¥ï¸ Streamlit GUI** | User-friendly interface for ingestion, querying, and monitoring |

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          Financial Report Analyzer                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  Streamlit    â”‚    â”‚   FastAPI        â”‚    â”‚   SEC EDGAR           â”‚     â”‚
â”‚  â”‚  Frontend     â”‚â”€â”€â”€â–¶â”‚   Backend        â”‚â”€â”€â”€â–¶â”‚   (Data Source)       â”‚     â”‚
â”‚  â”‚  (Port 8501)  â”‚    â”‚   (Port 8000)    â”‚    â”‚                       â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚         â”‚                      â”‚                                            â”‚
â”‚         â”‚                      â–¼                                            â”‚
â”‚         â”‚             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                  â”‚
â”‚         â”‚             â”‚ Document         â”‚                                  â”‚
â”‚         â”‚             â”‚ Processor        â”‚                                  â”‚
â”‚         â”‚             â”‚ (Semantic Chunk) â”‚                                  â”‚
â”‚         â”‚             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                  â”‚
â”‚         â”‚                      â”‚                                            â”‚
â”‚         â–¼                      â–¼                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  Query        â”‚    â”‚   ChromaDB       â”‚    â”‚   Ollama              â”‚     â”‚
â”‚  â”‚  Engine       â”‚â—€â”€â”€â–¶â”‚   Vector Store   â”‚â—€â”€â”€â–¶â”‚   (Embeddings + LLM)  â”‚     â”‚
â”‚  â”‚  (RAG)        â”‚    â”‚   (Persistent)   â”‚    â”‚   (Port 11434)        â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Quick Start

### Prerequisites

- **Python 3.10+** (3.13 recommended)
- **Ollama** - Install from [ollama.ai](https://ollama.ai)

### 1. Clone and Setup

```bash
# Clone the repository
cd "/Volumes/T7 SHIELD/Finance report analyzer"

# Create virtual environment
python3 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Configure environment
cp .env.example .env
```

### 2. Pull Ollama Models

```bash
ollama pull nomic-embed-text    # Embeddings (274 MB)
ollama pull llama3:8b           # LLM (4.7 GB)
```

### 3. Start the Services

```bash
# Terminal 1: Start API Server
source venv/bin/activate
uvicorn api.main:app --reload --port 8000

# Terminal 2: Start Streamlit GUI
source venv/bin/activate
streamlit run streamlit_app.py
```

### 4. Open the App

- **Streamlit GUI**: http://localhost:8501
- **API Docs**: http://localhost:8000/docs

---

## ğŸ“– Usage

### Via Streamlit GUI

1. **Ingest Tab**: Enter ticker (e.g., `AAPL`) and reference date
2. **Query Tab**: Ask natural language questions about the filings
3. **Chunk Monitor**: View retrieved sources and relevance scores

### Via REST API

#### Ingest Filings
```bash
curl -X POST http://localhost:8000/ingest \
  -H "Content-Type: application/json" \
  -d '{"ticker": "AAPL", "reference_date": "2025-01-15"}'
```

#### Query Filings
```bash
curl -X POST http://localhost:8000/query \
  -H "Content-Type: application/json" \
  -d '{
    "ticker": "AAPL",
    "query": "What was the operating expense in fiscal year 2024?",
    "top_k": 5
  }'
```

---

## ğŸ”§ Configuration

Edit `.env` to customize:

```bash
# SEC EDGAR (Required for API compliance)
SEC_USER_AGENT=YourCompany your.email@example.com

# Ollama Settings
OLLAMA_HOST=http://localhost:11434
OLLAMA_LLM_MODEL=llama3:8b
OLLAMA_EMBED_MODEL=nomic-embed-text

# Storage Paths
CHROMA_DB_PATH=./data/chroma_db
FILINGS_CACHE_PATH=./data/filings
```

---

## ğŸ—‚ï¸ Project Structure

```
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ main.py              # FastAPI application & endpoints
â”‚   â””â”€â”€ schemas.py           # Pydantic request/response models
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py            # Configuration & environment variables
â”‚   â”œâ”€â”€ sec_downloader.py    # SEC EDGAR API integration
â”‚   â”œâ”€â”€ document_processor.py # Semantic chunking pipeline
â”‚   â”œâ”€â”€ embeddings.py        # Ollama embedding generation
â”‚   â”œâ”€â”€ vector_store.py      # ChromaDB operations
â”‚   â”œâ”€â”€ llm_interface.py     # Ollama LLM client
â”‚   â””â”€â”€ query_engine.py      # RAG orchestration
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ filings/             # Cached SEC filings (auto-created)
â”‚   â””â”€â”€ chroma_db/           # Vector database (auto-created)
â”œâ”€â”€ streamlit_app.py         # Streamlit GUI application
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ .env.example             # Environment template
â””â”€â”€ README.md
```

---

## ğŸ“Š API Reference

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/health` | GET | System health check (Ollama, ChromaDB status) |
| `/ingest` | POST | Download and vectorize SEC filings |
| `/query` | POST | RAG query with source attribution |
| `/query/stream` | POST | Streaming RAG response |
| `/collections` | GET | List all ingested tickers |
| `/collections/{ticker}` | GET | Get collection statistics |
| `/collections/{ticker}` | DELETE | Remove ticker data |

Full OpenAPI documentation available at `/docs` when server is running.

---

## ğŸ’¡ Example Queries

```
"What was Apple's total revenue in fiscal year 2024?"
"Compare R&D expenses in Q2 2024 vs Q2 2023"
"Summarize the key risk factors from the latest 10-K"
"What was the YoY growth in operating income?"
"What are the main differences between 2023 and 2024 annual reports?"
"Explain the company's revenue recognition policy"
```

---

## ğŸ”¬ Technical Details

### Chunking Strategy

| Filing Section | Min Chunk Size | Rationale |
|----------------|----------------|-----------|
| **Item 7 (MD&A)** | 2,500 chars | Preserves narrative context for analysis |
| **Item 1 (Business)** | 1,500 chars | Maintains company description coherence |
| **Other Sections** | 1,000 chars | Balances granularity and context |

### Model Configuration

| Component | Model | Dimensions | Notes |
|-----------|-------|------------|-------|
| **Embeddings** | nomic-embed-text | 768 | 137M parameters, optimized for retrieval |
| **LLM** | llama3:8b | - | Low temperature (0.1) for factual accuracy |

### Performance Considerations

- **Embedding Generation**: ~50-100ms per chunk
- **Query Latency**: 2-5s depending on context size
- **Storage**: ~1-2 MB per filing (vectorized)

---

## ğŸ› ï¸ Development

### Run Tests
```bash
pytest tests/ -v
```

### Type Checking
```bash
mypy src/ api/
```

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- [SEC EDGAR](https://www.sec.gov/edgar) for public filing data
- [Ollama](https://ollama.ai) for local LLM infrastructure
- [ChromaDB](https://www.trychroma.com/) for vector storage
- [FastAPI](https://fastapi.tiangolo.com/) for the API framework
- [Streamlit](https://streamlit.io/) for the frontend framework
